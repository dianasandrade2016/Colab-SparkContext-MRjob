{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PROJ_AWS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPcoK046UPJbzAKe8lB1vzA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dianasandrade2016/AWS/blob/main/Projeto_AWS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74PMPW5607B8",
        "outputId": "56ca66de-c908-4102-f558-e2e1c58c832d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.0)\n",
            "Requirement already satisfied: py4j==0.10.9.2 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install mrjob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKf0UZdh1L9v",
        "outputId": "044c1764-afe9-4d97-8b90-1b1bd8b8e54b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mrjob in /usr/local/lib/python3.7/dist-packages (0.7.4)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from mrjob) (3.13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%file test1.py\n",
        "import re\n",
        "import os\n",
        "import sys\n",
        "from operator import add\n",
        "from mrjob.job import MRJob\n",
        "\n",
        "WORD_RE = re.compile(r\"[\\w']+\")\n",
        "\n",
        "class MRSparkWordcount(MRJob):\n",
        "\n",
        "    def spark(self, input_path, output_path):\n",
        "        from pyspark import SparkContext\n",
        "        sc = SparkContext(appName='mrjob Spark wordcount script')\n",
        "        lines = sc.textFile(input_path)\n",
        "\n",
        "        counts = (\n",
        "            lines.flatMap(self.get_words)\n",
        "            .map(lambda word: (word, 1))\n",
        "            .reduceByKey(add))\n",
        "\n",
        "        rdd = counts.map(lambda x: (x[1],x[0])).sortByKey(False)\n",
        "        teste = rdd.take(20000)\n",
        "\n",
        "        convert = sc.parallelize(teste)\n",
        "        for element in convert.collect():\n",
        "          convert.saveAsTextFile(output_path)\n",
        "\n",
        "        sc.stop()\n",
        "        \n",
        "    def get_words(self, line):\n",
        "        return WORD_RE.findall(line)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    MRSparkWordcount.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCCQ6JYQ1QqN",
        "outputId": "c007b637-0e01-477b-e63d-421fd1855201"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting test1.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python test1.py sherlock.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H79d6y7W1l6f",
        "outputId": "4761e0d1-56e6-4774-bc19-3ae13b57e2e4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No configs found; falling back on auto-configuration\n",
            "No configs specified for inline runner\n",
            "Creating temp directory /tmp/test1.root.20211215.215017.620065\n",
            "Running step 1 of 1...\n",
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/lib/python3.7/dist-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "21/12/15 21:50:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "Traceback (most recent call last):\n",
            "  File \"test1.py\", line 42, in <module>\n",
            "    MRSparkWordcount.run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/mrjob/job.py\", line 616, in run\n",
            "    cls().execute()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/mrjob/job.py\", line 687, in execute\n",
            "    self.run_job()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/mrjob/job.py\", line 636, in run_job\n",
            "    runner.run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/mrjob/runner.py\", line 503, in run\n",
            "    self._run()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/mrjob/sim.py\", line 161, in _run\n",
            "    self._run_step(step, step_num)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/mrjob/sim.py\", line 168, in _run_step\n",
            "    self._run_step_on_spark(step, step_num)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/mrjob/inline.py\", line 184, in _run_step_on_spark\n",
            "    task.execute()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/mrjob/job.py\", line 684, in execute\n",
            "    self.run_spark(self.options.step_num)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/mrjob/job.py\", line 912, in run_spark\n",
            "    spark_method(input_path, output_path)\n",
            "  File \"test1.py\", line 33, in spark\n",
            "    convert.saveAsTextFile(output_path)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pyspark/rdd.py\", line 1828, in saveAsTextFile\n",
            "    keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\", line 1310, in __call__\n",
            "    answer, self.gateway_client, self.target_id, self.name)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\", line 328, in get_return_value\n",
            "    format(target_id, \".\", name), value)\n",
            "py4j.protocol.Py4JJavaError: An error occurred while calling o115.saveAsTextFile.\n",
            ": org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/tmp/test1.root.20211215.215017.620065/output already exists\n",
            "\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n",
            "\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:299)\n",
            "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n",
            "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
            "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
            "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n",
            "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
            "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
            "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n",
            "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
            "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
            "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\n",
            "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
            "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
            "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1578)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
            "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
            "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1578)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1564)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
            "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
            "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1564)\n",
            "\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:551)\n",
            "\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:550)\n",
            "\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
            "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
            "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
            "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}